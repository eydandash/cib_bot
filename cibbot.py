from email.mime import message
import chainlit as cl
import httpx
import logging
import json
import requests
from vectorDB.qdrant_scripts import search_qdrant
from generation.llm import build_contextual_prompt, call_mistral_stream
from qdrant_client import QdrantClient

# ------------------ Logging Setup ------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)
# ---------------------------------------------------

# ------------------ Configuration ------------------
OLLAMA_URL = "http://host.docker.internal:11434/api/chat"
OLLAMA_HEALTH_URL = "http://host.docker.internal:11434/api/tags"

# Qdrant configuration - use host.docker.internal for Docker environment
QDRANT_HOST = "host.docker.internal"  # Changed from localhost for Docker
QDRANT_PORT = 6333
COLLECTION_NAME = "cib_financial_statements"

# Initialize Qdrant client
try:
    qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
except Exception as e:
    # Fallback to localhost if Docker internal host fails
    logger.warning(f"Failed to connect to {QDRANT_HOST}, trying localhost: {e}")
    qdrant_client = QdrantClient(host="localhost", port=QDRANT_PORT)

WELCOME_MSG = (
    "ğŸ‘‹ Hello and welcome! I'm your assistant here to help you explore and understand CIB's financial statements. "
    "Whether you're looking for specific figures, trends, or just trying to make sense of the reports â€” I'm here to support you every step of the way.\n\n"
    "Feel free to ask me any questions in English or Arabic. Just type what youâ€™re curious about!\n\n"
    "Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ø³ØªÙƒØ´Ø§Ù ÙˆÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ù„Ù„Ø¨Ù†Ùƒ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ø§Ù„Ø¯ÙˆÙ„ÙŠ. "
    "Ø³ÙˆØ§Ø¡ ÙƒÙ†Øª ØªØ¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù… Ù…Ø­Ø¯Ø¯Ø©ØŒ Ø£Ùˆ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø¹Ø§Ù…Ø©ØŒ Ø£Ùˆ ØªØ­Ø§ÙˆÙ„ ÙÙ‚Ø· ÙÙ‡Ù… Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± â€” Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.\n\n"
    "Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ Ø·Ø±Ø­ Ø£ÙŠ Ø³Ø¤Ø§Ù„ØŒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©. ÙÙ‚Ø· Ø§ÙƒØªØ¨ Ù…Ø§ ÙŠØ¯ÙˆØ± ÙÙŠ Ø°Ù‡Ù†Ùƒ!"
)

@cl.on_chat_start
async def start():
    # Check Ollama/Mistral connectivity
    try:
        async with httpx.AsyncClient(timeout=3.0) as client:
            response = await client.get(OLLAMA_HEALTH_URL)
            response.raise_for_status()
        logger.info("âœ… Mistral/Ollama is up and reachable.")
    except httpx.RequestError as e:
        logger.error("âŒ Could not connect to Mistral/Ollama!")
        await cl.Message(content="âš ï¸ Mistral (Ollama) is not reachable. Please make sure the model is running.").send()
        return

    # Check Qdrant connectivity
    try:
        collections = qdrant_client.get_collections()
        collection_exists = any(col.name == COLLECTION_NAME for col in collections.collections)
        if collection_exists:
            collection_info = qdrant_client.get_collection(COLLECTION_NAME)
            logger.info(f"âœ… Qdrant is up and collection '{COLLECTION_NAME}' has {collection_info.points_count} documents.")
        else:
            logger.warning(f"âš ï¸ Collection '{COLLECTION_NAME}' not found. Please make sure to run the data processing pipeline first.")
            await cl.Message(content=f"âš ï¸ Collection '{COLLECTION_NAME}' not found. Please process your financial documents first using the notebook.").send()
    except Exception as e:
        logger.error(f"âŒ Could not connect to Qdrant: {e}")
        await cl.Message(content="âš ï¸ Vector database is not reachable. Some features may be limited.").send()

    await cl.Message(content=WELCOME_MSG).send()

async def ask_ollama_with_context_stream(user_query: str):
    """
    Search vector database for relevant context and stream response from Mistral.
    
    Args:
        user_query (str): The user's question
        
    Yields:
        str: Tokens from the streaming response
    """
    try:
        # Step 1: Search for relevant context
        logger.info(f"ï¿½ Searching vector database for: {user_query}")
        
        try:
            search_results = search_qdrant(
                query=user_query,
                client=qdrant_client,
                collection_name=COLLECTION_NAME,
                top_k=3
            )
            
            if search_results:
                # Extract text from top results
                context_chunks = [result.payload["text"] for result in search_results]
                
                # Log source files for debugging
                source_files = [result.payload.get("file_name", "Unknown") for result in search_results]
                logger.info(f"ğŸ“„ Found context from files: {source_files}")
                
                # Step 2: Build contextual prompt
                prompt = build_contextual_prompt(user_query, context_chunks)
                logger.info(f"ğŸ“ Built contextual prompt with {len(context_chunks)} chunks")
                
            else:
                # Fallback: answer without context
                logger.warning("âš ï¸ No relevant context found, answering without context")
                prompt = f"""You are a financial assistant for the Commercial International Bank in Egypt. 
                
Question: {user_query}

Answer: I don't have specific information about that in my current database. Could you please rephrase your question or ask about CIB's financial statements, performance metrics, or other banking-related topics?"""
                
        except Exception as search_error:
            logger.error(f"âŒ Vector search failed: {search_error}")
            # Fallback: answer without context
            prompt = f"""You are a financial assistant for the Commercial International Bank in Egypt. 
            
Question: {user_query}

Answer: I'm experiencing some technical difficulties accessing the financial database. Could you please try again or ask a general question about CIB?"""
        
        # Step 3: Stream response from Mistral
        logger.info(f"ğŸ¤– Streaming response from Mistral...")
        async for token in async_call_mistral_stream(prompt):
            yield token
            
    except Exception as e:
        logger.error(f"âŒ Error in ask_ollama_with_context_stream: {e}")
        yield f"I apologize, but I'm experiencing technical difficulties. Please try again later. Error: {str(e)}"


async def async_call_mistral_stream(prompt: str):
    """
    Async wrapper for the streaming Mistral function.
    
    Args:
        prompt (str): The prompt to send to Mistral
        
    Yields:
        str: Tokens from the streaming response
    """
    import asyncio
    import concurrent.futures
    
    # Run the synchronous streaming function in a thread with correct URL
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(lambda: list(call_mistral_stream(prompt, OLLAMA_URL)))
        
        # Get the result and yield tokens
        try:
            tokens = await asyncio.wrap_future(future)
            for token in tokens:
                yield token
        except Exception as e:
            logger.error(f"âŒ Async streaming error: {e}")
            yield "Error generating response. Please try again."

@cl.on_message
async def handle_message(message: cl.Message):
    logger.info(f"ğŸ’¬ User message: {message.content}")

    # Initialize history if it doesn't exist
    if not hasattr(cl.user_session, "history"):
        cl.user_session.history = []

    cl.user_session.history.append({"role": "user", "content": message.content})

    # Stream response with vector search context
    msg = cl.Message(content="")
    async for token in ask_ollama_with_context_stream(message.content):
        await msg.stream_token(token)
    
    cl.user_session.history.append({"role": "assistant", "content": msg.content})
    await msg.send()
